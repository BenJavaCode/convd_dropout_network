{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "13ae3cf3d6d148b6ba8b6e83fa558214": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_50e69d8d697b48b1a68d1acdc3244cc1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5b5388c010fc4453946ddc92b13837a1",
              "IPY_MODEL_267cefe2b3bb4068a1734616f049bdba"
            ]
          }
        },
        "50e69d8d697b48b1a68d1acdc3244cc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5b5388c010fc4453946ddc92b13837a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4f342b751cfd4aaea8e1d5c5da6fffdc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ab8445415faf4fffaf8348771b1f4802"
          }
        },
        "267cefe2b3bb4068a1734616f049bdba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_03120a9611a64b1999581b2a63c49134",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:06&lt;00:00, 24888460.15it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_07b2f2a980fc441990405760d6aad4a6"
          }
        },
        "4f342b751cfd4aaea8e1d5c5da6fffdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ab8445415faf4fffaf8348771b1f4802": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "03120a9611a64b1999581b2a63c49134": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "07b2f2a980fc441990405760d6aad4a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MptaLUi-g4Ir"
      },
      "source": [
        "# Simple neural network, that uses convolutions and downsampling to genneralize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8K-lz8qqi9V"
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLkCB4zmqr5i"
      },
      "source": [
        "data_path = r\"/content/drive/My Drive/cifar10/data\"\n",
        "data_path = data_path.replace('\\\\', '/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra2GM8xMq7BU"
      },
      "source": [
        "class_names = ['airplane','automobile','bird','cat','deer',\n",
        "               'dog','frog','horse','ship','truck']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2uJrymQq-O2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "13ae3cf3d6d148b6ba8b6e83fa558214",
            "50e69d8d697b48b1a68d1acdc3244cc1",
            "5b5388c010fc4453946ddc92b13837a1",
            "267cefe2b3bb4068a1734616f049bdba",
            "4f342b751cfd4aaea8e1d5c5da6fffdc",
            "ab8445415faf4fffaf8348771b1f4802",
            "03120a9611a64b1999581b2a63c49134",
            "07b2f2a980fc441990405760d6aad4a6"
          ]
        },
        "outputId": "4b0fb7a8-1886-4045-f675-6f59bc9a012c"
      },
      "source": [
        "tensor_cifar10 = datasets.CIFAR10(data_path, \n",
        "                                  train = True, \n",
        "                                  download = True, \n",
        "                                  transform = transforms.ToTensor())\n",
        "\n",
        "\n",
        "tensor_cifar10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/drive/My Drive/cifar10/data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13ae3cf3d6d148b6ba8b6e83fa558214",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /content/drive/My Drive/cifar10/data/cifar-10-python.tar.gz to /content/drive/My Drive/cifar10/data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset CIFAR10\n",
              "    Number of datapoints: 50000\n",
              "    Root location: /content/drive/My Drive/cifar10/data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9ksUdVbrDFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5166eebe-6012-4618-9122-f60e6f545205"
      },
      "source": [
        "imgs = torch.stack([img_t for img_t, label in tensor_cifar10], dim=3)\n",
        "#torch.stack(tensors, dim=0, *, out=None) → Tensor\n",
        "#Concatenates sequence of tensors along a new dimension.\n",
        "#All tensors need to be of the same size."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyXGRsHerJk0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d2de3ad-0798-4e52-b9d0-96e893100ac2"
      },
      "source": [
        "imgs.shape #rgb, h, w, datapoints"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 32, 32, 50000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5q_GRFdrLen",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be4b8759-1657-486d-a599-a66dc1c90743"
      },
      "source": [
        "mean = imgs.view(3, -1).mean(dim=1)\n",
        "mean\n",
        "#Recall that view(3, -1) keeps the three channels and\n",
        "#merges all the remaining dimensions into one, figuring\n",
        "#out the appropriate size. Here our 3 × 32 × 32 x 50000 image is\n",
        "#transformed into a 3 × 1,024 vector, and then the mean\n",
        "#is taken over the 32 × 32 x 50000 elements of each channel"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.4914, 0.4822, 0.4465])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKqQ91zqrNUj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44782df5-ec5d-4115-ea8e-b3c34e7e910c"
      },
      "source": [
        "std = imgs.view(3, -1).std(dim=1) # dim is which dimension index to do the calc on\n",
        "std"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2470, 0.2435, 0.2616])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmK7SaKlrPH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf3f3f2b-af91-488a-f9ab-a4b67bf0427c"
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "data_path = data_path\n",
        "cifar10 = datasets.CIFAR10(\n",
        "    data_path, train=True, download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean,std)\n",
        "    ]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtNDvxcdrRR8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd26877e-4965-429e-ea84-cb8f2ad487fe"
      },
      "source": [
        "cifar10_val = datasets.CIFAR10(\n",
        "    data_path, train=False, download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4Tr1hYVuoJ2"
      },
      "source": [
        "class_names = ['airplane', 'bird']\n",
        "label_map = {0: 0, 2: 1}\n",
        "\n",
        "cifar2 = [(img, label_map[label])\n",
        "          for img, label in cifar10\n",
        "          if label in [0, 2]]\n",
        "\n",
        "cifar2_val = [(img, label_map[label])\n",
        "              for img, label in cifar10_val\n",
        "              if label in [0, 2]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk96bmmFrizR"
      },
      "source": [
        "#convolutionary feed forward network that uses dropout to combat overfitting\n",
        "import torch.nn.functional as fn\n",
        "\n",
        "class NetDropout(nn.Module):\n",
        "    def __init__(self, n_chans1=32):\n",
        "        super().__init__()\n",
        "        self.n_chans1 = n_chans1\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "        self.conv1_dropout = nn.Dropout2d(p=0.4) # probability for dropout 40%\n",
        "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
        "                               padding=1)\n",
        "        self.conv2_dropout = nn.Dropout2d(p=0.4) # probability for dropout 40%\n",
        "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2) # convolution, tanh activation, downsample by half\n",
        "        out = self.conv1_dropout(out) # apply\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = self.conv2_dropout(out) # apply\n",
        "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3KJp921rmT3"
      },
      "source": [
        "import datetime\n",
        "\n",
        "def training_loop(n_epochs, optimizer, scheduler, model, loss_fn, train_loader):\n",
        "    for epoch in range(1, n_epochs + 1):  #  starting from  1 instead of 0\n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:  # Using the train loader\n",
        "\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(imgs)  # applying the model on the img data in train loader\n",
        "            \n",
        "            loss = loss_fn(outputs, labels)  # computing the loss\n",
        "\n",
        "            optimizer.zero_grad()  # clearing the gradients from last pass\n",
        "            \n",
        "            loss.backward()  # compute gradients on the parameters\n",
        "            \n",
        "            optimizer.step()  # optimize model based on gradients\n",
        "\n",
        "            loss_train += loss.item()  # sums losses over epoch, item() is to escape gradients\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if epoch == 1 or epoch % 10 == 0:\n",
        "            print('{} Epoch {}, Training loss {}'.format(\n",
        "                datetime.datetime.now(), epoch,\n",
        "                loss_train / len(train_loader)))  # Divides by the length of the\n",
        "                                                  #training data loader to get the\n",
        "                                                #average loss per batch."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dlcr8xGmrsAt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ed949c1-296e-4cc7-98d1-45bac66a8542"
      },
      "source": [
        "device = (torch.device('cuda') if torch.cuda.is_available()\n",
        "else torch.device('cpu'))\n",
        "print(f\"Training on device {device}.\")\n",
        "use_cuda = torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on device cuda.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSaabQBZr0t-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c85eef1-08c1-4be5-fbe2-f0451f1e1b2f"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, \n",
        "                                           batch_size=64, \n",
        "                                           pin_memory=use_cuda, #Pinned memory transfers to GPU quickly\n",
        "                                           num_workers = 2, # collab cpu has 2 threads..\n",
        "                                           shuffle=True)  # The DataLoader batches up the examples of our cifar2 dataset.\n",
        "                                           #Shuffling randomizes the order of the examples from the dataset.\n",
        "\n",
        "model = NetDropout()  #  instantiate network\n",
        "model = model.to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum = 0.82)  #  instantiate optimizer with model parameters and learning rate\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, gamma=0.8, step_size=20)# Decay LR by a factor of gamma every step_size\n",
        "#Decays the learning rate of each parameter group by gamma every step_size epochs. Notice that such decay can happen. When last_epoch=-1, sets initial lr as lr.\n",
        "loss_fn = nn.CrossEntropyLoss()  #  loss function\n",
        "\n",
        "training_loop(  # call training loop\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    scheduler = exp_lr_scheduler,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-21 22:11:22.913409 Epoch 1, Training loss 0.5112302293823023\n",
            "2020-11-21 22:11:30.044592 Epoch 10, Training loss 0.32122074589607824\n",
            "2020-11-21 22:11:38.131233 Epoch 20, Training loss 0.27138395355955053\n",
            "2020-11-21 22:11:46.039676 Epoch 30, Training loss 0.23438411618873572\n",
            "2020-11-21 22:11:54.025223 Epoch 40, Training loss 0.21414515015425956\n",
            "2020-11-21 22:12:02.016983 Epoch 50, Training loss 0.1921895585337262\n",
            "2020-11-21 22:12:10.286756 Epoch 60, Training loss 0.17706134568923598\n",
            "2020-11-21 22:12:18.197728 Epoch 70, Training loss 0.16045254228079014\n",
            "2020-11-21 22:12:25.887738 Epoch 80, Training loss 0.1508544286724868\n",
            "2020-11-21 22:12:34.113153 Epoch 90, Training loss 0.13790187232528522\n",
            "2020-11-21 22:12:42.147765 Epoch 100, Training loss 0.12926792127975992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nk8CzWusIID"
      },
      "source": [
        "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, pin_memory=use_cuda, num_workers = 2, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7WLFpHKsUGy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "729b9ea0-3875-42d5-be31-abe37d68d7b8"
      },
      "source": [
        "\n",
        "def validate(model, train_loader, val_loader):\n",
        "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():  # we do not want to update parameters\n",
        "            for imgs, labels in loader:\n",
        "                imgs = imgs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs, dim=1) # give us the index of the highest value as output\n",
        "                total += labels.shape[0]  # counts number of examples, so total is increased by batch size\n",
        "                correct += int((predicted == labels).sum())  # Comparing the predicted class that had the\n",
        "                                                            #maximum probability and the ground-truth\n",
        "                                                            #labels, we first get a Boolean array. Taking the\n",
        "                                                            #sum gives the number of items in the batch\n",
        "                                                            #where the prediction and ground truth agree.\n",
        "\n",
        "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
        "\n",
        "validate(model, train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy train: 0.95\n",
            "Accuracy val: 0.89\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VV-Xwr9sUhn"
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}